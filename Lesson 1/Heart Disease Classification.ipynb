{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Heart Disease using Machine Learning\n",
    "\n",
    "This notebook will introduce some foundation machine learning and data science concepts by exploring the problem of heart disease **classification**.\n",
    "\n",
    "It is intended to be an end-to-end example of what a data science and machine learning **proof of concept** might look like.\n",
    "\n",
    "## What is classification?\n",
    "\n",
    "Classification involves deciding whether a sample is part of one class or another (**single-class classification**). If there are multiple class options, it's referred to as **multi-class classification**.\n",
    "\n",
    "\n",
    "## What we'll end up with\n",
    "\n",
    "Since we already have a dataset, we'll approach the problem with the following machine learning modelling framework.\n",
    "\n",
    "| <img src=\"../images/ml101-6-step-ml-framework.png\" width=500/> | \n",
    "|:--:| \n",
    "| 6 Step Machine Learning Modelling Framework |\n",
    "\n",
    "More specifically, we'll look at the following topics.\n",
    "\n",
    "* **Model training** - create model(s) to learn to predict a target variable based on other variables.\n",
    "* **Model evaluation** - evaluating a models predictions using problem-specific evaluation metrics. \n",
    "* **Model comparison** - comparing several different models to find the best one.\n",
    "* **Model fine-tuning** - once we've found a good model, how can we improve it?\n",
    "* **Feature importance** - since we're predicting the presence of heart disease, are there some things which are more important for prediction?\n",
    "* **Cross-validation** - if we do build a good model, can we be sure it will work on unseen data?\n",
    "* **Reporting what we've found** - if we had to present our work, what would we show someone?\n",
    "\n",
    "To work through these topics, we'll use pandas, Matplotlib and NumPy for data anaylsis, as well as, Scikit-Learn for machine learning and modelling tasks.\n",
    "\n",
    "| <img src=\"../images/supervised-projects-6-step-ml-framework-tools-highlight.png\" width=500/> | \n",
    "|:--:| \n",
    "| Tools which can be used for each step of the machine learning modelling process. |\n",
    "\n",
    "We'll work through each step and by the end of the notebook, we'll have a handful of models, all which can predict whether or not a person has heart disease based on a number of different parameters at a considerable accuracy. \n",
    "\n",
    "You'll also be able to describe which parameters are more indicative than others, for example, sex may be more important than age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem Definition\n",
    "In our case, the problem we will be exploring is **binary classification** (a sample can only be one of two things). \n",
    "\n",
    "This is because we're going to be using a number of differnet **features** (pieces of information) about a person to predict whether they have heart disease or not.\n",
    "\n",
    "In a statement,\n",
    "\n",
    "> Given clinical parameters about a patient, can we predict whether or not they have heart disease?\n",
    "\n",
    "## 2. Data\n",
    "\n",
    "What you'll want to do here is dive into the data your problem definition is based on. This may involve, sourcing, defining different parameters, talking to experts about it and finding out what you should expect.\n",
    "\n",
    "The original data came from the [Cleveland database](https://archive.ics.uci.edu/ml/datasets/heart+Disease) from UCI Machine Learning Repository.\n",
    "\n",
    "Howevever, we've downloaded it in a formatted way from [Kaggle](https://www.kaggle.com/datasets/sumaiyatasmeem/heart-disease-classification-dataset).\n",
    "\n",
    "The original database contains 76 attributes, but here only 14 attributes will be used. **Attributes** (also called **features**) are the variables what we'll use to predict our **target variable**.\n",
    "\n",
    "Attributes and features are also referred to as **independent variables** and a target variable can be referred to as a **dependent variable**.\n",
    "\n",
    "> We use the independent variables to predict our dependent variable.\n",
    "\n",
    "Or in our case, the independent variables are a patients different medical attributes and the dependent variable is whether or not they have heart disease.\n",
    "\n",
    "## 3. Evaluation\n",
    "\n",
    "The evaluation metric is something you might define at the start of a project.\n",
    "\n",
    "Since machine learning is very experimental, you might say something like, \n",
    "\n",
    "> If we can reach 95% accuracy at predicting whether or not a patient has heart disease during the proof of concept, we'll pursure this project.\n",
    "\n",
    "The reason this is helpful is it provides a rough goal for a machine learning engineer or data scientist to work towards.\n",
    "\n",
    "However, due to the nature of experimentation, the evaluation metric may change over time.\n",
    "\n",
    "## 4. Features\n",
    "\n",
    "Features are different parts of the data. During this step, you'll want to start finding out what you can about the data.\n",
    "\n",
    "One of the most common ways to do this, is to create a **data dictionary**.\n",
    "\n",
    "### Heart Disease Data Dictionary\n",
    "\n",
    "A data dictionary describes the data you're dealing with. Not all datasets come with them so this is where you may have to do your research or ask a **subject matter expert** (someone who knows about the data) for more.\n",
    "\n",
    "The following are the features we'll use to predict our target variable (heart disease or no heart disease).\n",
    "\n",
    "1. age - age in years \n",
    "2. sex - (1 = male; 0 = female) \n",
    "3. cp - chest pain type \n",
    "    * 0: Typical angina: chest pain related decrease blood supply to the heart\n",
    "    * 1: Atypical angina: chest pain not related to heart\n",
    "    * 2: Non-anginal pain: typically esophageal spasms (non heart related)\n",
    "    * 3: Asymptomatic: chest pain not showing signs of disease\n",
    "4. trestbps - resting blood pressure (in mm Hg on admission to the hospital)\n",
    "    * anything above 130-140 is typically cause for concern\n",
    "5. chol - serum cholestoral in mg/dl \n",
    "    * serum = LDL + HDL + .2 * triglycerides\n",
    "    * above 200 is cause for concern\n",
    "6. fbs - (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false) \n",
    "    * '>126' mg/dL signals diabetes\n",
    "7. restecg - resting electrocardiographic results\n",
    "    * 0: Nothing to note\n",
    "    * 1: ST-T Wave abnormality\n",
    "        - can range from mild symptoms to severe problems\n",
    "        - signals non-normal heart beat\n",
    "    * 2: Possible or definite left ventricular hypertrophy\n",
    "        - Enlarged heart's main pumping chamber\n",
    "8. thalach - maximum heart rate achieved \n",
    "9. exang - exercise induced angina (1 = yes; 0 = no) \n",
    "10. oldpeak - ST depression induced by exercise relative to rest \n",
    "    * looks at stress of heart during excercise\n",
    "    * unhealthy heart will stress more\n",
    "11. slope - the slope of the peak exercise ST segment\n",
    "    * 0: Upsloping: better heart rate with excercise (uncommon)\n",
    "    * 1: Flatsloping: minimal change (typical healthy heart)\n",
    "    * 2: Downslopins: signs of unhealthy heart\n",
    "12. ca - number of major vessels (0-3) colored by flourosopy \n",
    "    * colored vessel means the doctor can see the blood passing through\n",
    "    * the more blood movement the better (no clots)\n",
    "13. thal - thalium stress result\n",
    "    * 1,3: normal\n",
    "    * 6: fixed defect: used to be defect but ok now\n",
    "    * 7: reversable defect: no proper blood movement when excercising \n",
    "14. target - have disease or not (1=yes, 0=no) (= the predicted attribute)\n",
    "\n",
    "**Note:** No personal identifiable information (PPI) can be found in the dataset.\n",
    "\n",
    "It's a good idea to save these to a Python dictionary or in an external file, so we can look at them later without coming back here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the tools\n",
    "\n",
    "At the start of any project, it's custom to see the required libraries imported in a big chunk like you can see below.\n",
    "\n",
    "However, in practice, your projects may import libraries as you go. After you've spent a couple of hours working on your problem, you'll probably want to do some tidying up. This is where you may want to consolidate every library you've used at the top of your notebook (like the cell below).\n",
    "\n",
    "The libraries you use will differ from project to project. But there are a few which will you'll likely take advantage of during almost every structured data project. \n",
    "\n",
    "* [pandas](https://pandas.pydata.org/) for data analysis.\n",
    "* [NumPy](https://numpy.org/) for numerical operations.\n",
    "* [Matplotlib](https://matplotlib.org/)/[seaborn](https://seaborn.pydata.org/) for plotting or data visualization.\n",
    "* [Scikit-Learn](https://scikit-learn.org/stable/) for machine learning modelling and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python310\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated: Mon Mar 11 20:52:37 2024\n"
     ]
    }
   ],
   "source": [
    "# Regular EDA and plotting libraries\n",
    "import numpy as np # np is short for numpy\n",
    "import pandas as pd # pandas is so commonly used, it's shortened to pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # seaborn gets shortened to sns\n",
    "\n",
    "# We want our plots to appear in the notebook\n",
    "%matplotlib inline \n",
    "\n",
    "## Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "## Model evaluators\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "# from sklearn.metrics import plot_roc_curve # note: this was changed in Scikit-Learn 1.2+ to be \"RocCurveDisplay\" (see below)\n",
    "from sklearn.metrics import RocCurveDisplay # new in Scikit-Learn 1.2+\n",
    "\n",
    "# Print last updated\n",
    "import time\n",
    "print(f\"Last updated: {time.asctime()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "There are many different kinds of ways to store data. The typical way of storing **tabular data**, data similar to what you'd see in an Excel file is in `.csv` format. `.csv` stands for comma seperated values.\n",
    "\n",
    "Pandas has a built-in function to read `.csv` files called `read_csv()` which takes the file pathname of your `.csv` file. You'll likely use this a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 14)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/heart-disease.csv\") # 'DataFrame' shortened to 'df'\n",
    "df.shape # (rows, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration (exploratory data analysis or EDA)\n",
    "\n",
    "Once you've imported a dataset, the next step is to explore. There's no set way of doing this. But what you should be trying to do is become more and more familiar with the dataset.\n",
    "\n",
    "Compare different columns to each other, compare them to the target variable. Refer back to your **data dictionary** and remind yourself of what different columns mean.\n",
    "\n",
    "Your goal is to become a subject matter expert on the dataset you're working with. So if someone asks you a question about it, you can give them an explanation and when you start building models, you can sound check them to make sure they're not performing too well (**overfitting**) or why they might be performing poorly (**underfitting**).\n",
    "\n",
    "Since EDA has no real set methodolgy, the following is a short check list you might want to walk through:\n",
    "\n",
    "1. What question(s) are you trying to solve (or prove wrong)?\n",
    "2. What kind of data do you have and how do you treat different types?\n",
    "3. What’s missing from the data and how do you deal with it?\n",
    "4. Where are the outliers and why should you care about them?\n",
    "5. How can you add, change or remove features to get more out of your data?\n",
    "\n",
    "Once of the quickest and easiest ways to check your data is with the `head()` function. Calling it on any dataframe will print the top 5 rows, `tail()` calls the bottom 5. You can also pass a number to them like `head(10)` to show the top 10 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the top 5 rows of our dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many positive (1) and negative (0) samples we have in our dataframe\n",
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since these two values are close to even, our `target` column can be considered **balanced**. An **unbalanced** target column, meaning some classes have far more samples, can be harder to model than a balanced set. Ideally, all of your target classes have the same number of samples.\n",
    "\n",
    "If you'd prefer these values in percentages, `value_counts()` takes a parameter, `normalize` which can be set to true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the target column value counts by calling the `plot()` function and telling it what kind of plot we'd like, in this case, bar is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the value counts with a bar graph\n",
    "df.target.value_counts().plot(kind=\"bar\", color=[\"salmon\", \"lightblue\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`df.info()` shows a quick insight to the number of missing values you have and what type of data your working with.\n",
    "\n",
    "In our case, there are no missing values and all of our columns are numerical in nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to get some quick insights on your dataframe is to use `df.describe()`. `describe()` shows a range of different metrics about your numerical columns such as mean, max and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modeling\n",
    "\n",
    "We've explored the data, now we'll try to use machine learning to predict our target variable based on the 13 independent variables.\n",
    "\n",
    "Remember our problem?\n",
    "\n",
    "> Given clinical parameters about a patient, can we predict whether or not they have heart disease?\n",
    "\n",
    "That's what we'll be trying to answer.\n",
    "\n",
    "And remember our evaluation metric?\n",
    "\n",
    "> If we can reach 95% accuracy at predicting whether or not a patient has heart disease during the proof of concept, we'll pursure this project.\n",
    "\n",
    "That's what we'll be aiming for.\n",
    "\n",
    "But before we build a model, we have to get our dataset ready.\n",
    "\n",
    "Let's look at it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're trying to predict our target variable using all of the other variables.\n",
    "\n",
    "To do this, we'll split the target variable from the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everything except target variable\n",
    "X = df.drop(\"target\", axis=1)\n",
    "\n",
    "# Target variable\n",
    "y = df.target.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see our new variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent variables (no target column)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Targets\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and test split\n",
    "\n",
    "Now comes one of the most important concepts in machine learning, the **training/test split**.\n",
    "\n",
    "This is where you'll split your data into a **training set** and a **test set**.\n",
    "\n",
    "You use your training set to train your model and your test set to test it.\n",
    "\n",
    "The test set must remain separate from your training set.\n",
    "\n",
    "#### Why not use all the data to train a model?\n",
    "\n",
    "Let's say you wanted to take your model into the hospital and start using it on patients. How would you know how well your model goes on a new patient not included in the original full dataset you had?\n",
    "\n",
    "This is where the test set comes in. It's used to mimic taking your model to a real environment as much as possible.\n",
    "\n",
    "And it's why it's important to never let your model learn from the test set, it should only be evaluated on it.\n",
    "\n",
    "To split our data into a training and test set, we can use Scikit-Learn's [`train_test_split()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) and feed it our independent and dependent variables (`X` & `y`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split into train & test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, # independent variables \n",
    "                                                    y, # dependent variable\n",
    "                                                    test_size = 0.2) # percentage of data to use for test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `test_size` parameter is used to tell the `train_test_split()` function how much of our data we want in the test set.\n",
    "\n",
    "A rule of thumb is to use 80% of your data to train on and the other 20% to test on. \n",
    "\n",
    "For our problem, a train and test set are enough. But for other problems, you could also use a validation (train/validation/test) set or cross-validation (we'll see this in a second).\n",
    "\n",
    "But again, each problem will differ. The post, [How (and why) to create a good validation set](https://www.fast.ai/2017/11/13/validation-sets/) by Rachel Thomas is a good place to go to learn more.\n",
    "\n",
    "Let's look at our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful, we can see we're using 242 samples to train on. Let's look at our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test, len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we've got 61 examples we'll test our model(s) on. Let's build some."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model choices\n",
    "\n",
    "Now we've got our data prepared, we can start to fit models. We'll be using the following and comparing their results.\n",
    "\n",
    "1. Logistic Regression - [`LogisticRegression()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "2. K-Nearest Neighbors - [`KNeighboursClassifier()`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "3. RandomForest - [`RandomForestClassifier()`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why these?\n",
    "\n",
    "If we look at the [Scikit-Learn algorithm cheat sheet](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html), we can see we're working on a classification problem and these are the algorithms it suggests (plus a few more).\n",
    "\n",
    "| <img src=\"../images/sklearn-ml-map-cheatsheet-heart-disease-ensemble.png\" alt=\"an example classification path using the Scikit-Learn machine learning model map\" width=500/> | \n",
    "|:--:| \n",
    "| An example path we can take using the Scikit-Learn Machine Learning Map |\n",
    "\n",
    "\"Wait, I don't see Logistic Regression and why not use LinearSVC?\"\n",
    "\n",
    "Good questions. \n",
    "\n",
    "I was confused too when I didn't see Logistic Regression listed as well because when you read the Scikit-Learn documentation on it, you can see it's [a model for classification](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression).\n",
    "\n",
    "And as for LinearSVC, let's pretend we've tried it, and it doesn't work, so we're following other options in the map.\n",
    "\n",
    "For now, knowing each of these algorithms inside and out is not essential.\n",
    "\n",
    "Machine learning and data science is an iterative practice. These algorithms are tools in your toolbox.\n",
    "\n",
    "In the beginning, on your way to becoming a practioner, it's more important to understand your problem (such as, classification versus regression) and then knowing what tools you can use to solve it.\n",
    "\n",
    "Since our dataset is relatively small, we can experiment to find algorithm performs best.\n",
    "\n",
    "All of the algorithms in the Scikit-Learn library use the same functions, for training a model, `model.fit(X_train, y_train)` and for scoring a model `model.score(X_test, y_test)`. `score()` returns the ratio of correct predictions (1.0 = 100% correct).\n",
    "\n",
    "Since the algorithms we've chosen implement the same methods for fitting them to the data as well as evaluating them, let's put them in a dictionary and create a which fits and scores them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put models in a dictionary\n",
    "models = {\"KNN\": KNeighborsClassifier(),\n",
    "          \"Logistic Regression\": LogisticRegression(), \n",
    "          \"Random Forest\": RandomForestClassifier()}\n",
    "\n",
    "# Create function to fit and score models\n",
    "def fit_and_score(models, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Fits and evaluates given machine learning models.\n",
    "    models : a dict of different Scikit-Learn machine learning models\n",
    "    X_train : training data\n",
    "    X_test : testing data\n",
    "    y_train : labels assosciated with training data\n",
    "    y_test : labels assosciated with test data\n",
    "    \"\"\"\n",
    "    # Random seed for reproducible results\n",
    "    np.random.seed(42)\n",
    "    # Make a list to keep model scores\n",
    "    model_scores = {}\n",
    "    # Loop through models\n",
    "    for name, model in models.items():\n",
    "        # Fit the model to the data\n",
    "        model.fit(X_train, y_train)\n",
    "        # Evaluate the model and append its score to model_scores\n",
    "        model_scores[name] = model.score(X_test, y_test)\n",
    "    return model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores = fit_and_score(models=models,\n",
    "                             X_train=X_train,\n",
    "                             X_test=X_test,\n",
    "                             y_train=y_train,\n",
    "                             y_test=y_test)\n",
    "model_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "Since we've saved our models scores to a dictionary, we can plot them by first converting them to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_compare = pd.DataFrame(model_scores, index=['accuracy'])\n",
    "model_compare.T.plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful! We can't really see it from the graph but looking at the dictionary, the [LogisticRegression()](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model performs best.\n",
    "\n",
    "Since you've found the best model. Let's take it to the boss and show her what we've found.\n",
    "\n",
    "> **You:** I've found it!\n",
    "\n",
    "> **Her:** Nice one! What did you find?\n",
    "    \n",
    "> **You:** The best algorithm for prediting heart disease is a LogisticRegrssion!\n",
    "\n",
    "> **Her:** Excellent. I'm surprised the hyperparameter tuning is finished by now.\n",
    "\n",
    "> **You:** *wonders what **hyperparameter tuning** is*\n",
    "    \n",
    "> **You:** Ummm yeah, me too, it went pretty quick.\n",
    "    \n",
    "> **Her:** I'm very proud, how about you put together a **classification report** to show the team, and be sure to include a **confusion matrix**, and the **cross-validated precision**, **recall** and **F1 scores**. I'd also be curious to see what **features are most important**. Oh and don't forget to include a **ROC curve**.\n",
    "    \n",
    "> **You:** *asks self, \"what are those???\"*\n",
    "    \n",
    "> **You:** Of course! I'll have to you by tomorrow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, there were a few words in there which could sound made up to someone who's not a budding data scientist like yourself. But being the budding data scientist you are, you know data scientists make up words all the time.\n",
    "\n",
    "Let's briefly go through each before we see them in action.\n",
    "\n",
    "* **Hyperparameter tuning** - Each model you use has a series of dials you can turn to dictate how they perform. Changing these values may increase or decrease model performance.\n",
    "* **Feature importance** - If there are a large amount of features we're using to make predictions, do some have more importance than others? For example, for predicting heart disease, which is more important, sex or age?\n",
    "* [**Confusion matrix**](https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/) - Compares the predicted values with the true values in a tabular way, if 100% correct, all values in the matrix will be top left to bottom right (diagnol line).\n",
    "* [**Cross-validation**](https://scikit-learn.org/stable/modules/cross_validation.html) - Splits your dataset into multiple parts and train and tests your model on each part and evaluates performance as an average. \n",
    "* [**Precision**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score) - Proportion of true positives over total number of samples. Higher precision leads to less false positives.\n",
    "* [**Recall**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score) - Proportion of true positives over total number of true positives and false negatives. Higher recall leads to less false negatives.\n",
    "* [**F1 score**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score) - Combines precision and recall into one metric. 1 is best, 0 is worst.\n",
    "* [**Classification report**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) - Sklearn has a built-in function called `classification_report()` which returns some of the main classification metrics such as precision, recall and f1-score.\n",
    "* [**ROC Curve**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_score.html) - [Receiver Operating Characterisitc](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) is a plot of true positive rate versus false positive rate.\n",
    "* [**Area Under Curve (AUC)**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html) - The area underneath the ROC curve. A perfect model achieves a score of 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning and cross-validation\n",
    "\n",
    "To cook your favourite dish, you know to set the oven to 180 degrees and turn the grill on. But when your roommate cooks their favourite dish, they set use 200 degrees and the fan-forced mode. Same oven, different settings, different outcomes.\n",
    "\n",
    "The same can be done for machine learning algorithms. You can use the same algorithms but change the settings (hyperparameters) and get different results.\n",
    "\n",
    "But just like turning the oven up too high can burn your food, the same can happen for machine learning algorithms. You change the settings and it works so well, it **overfits** (does too well) the data.\n",
    "\n",
    "We're looking for the goldilocks model. One which does well on our dataset but also does well on unseen examples.\n",
    "\n",
    "To test different hyperparameters, you could use a **validation set** but since we don't have much data, we'll use **cross-validation**.\n",
    "\n",
    "The most common type of cross-validation is *k-fold*. It involves splitting your data into *k-fold's* and then testing a model on each. For example, let's say we had 5 folds (k = 5). This what it might look like.\n",
    "\n",
    "| <img src=\"../images/sklearn-cross-validation.png\" width=500/> | \n",
    "|:--:| \n",
    "| Normal train and test split versus 5-fold cross-validation |\n",
    "\n",
    "We'll be using this setup to tune the hyperparameters of some of our models and then evaluate them. We'll also get a few more metrics like **precision**, **recall**, **F1-score** and **ROC** at the same time.\n",
    "\n",
    "Here's the game plan:\n",
    "1. Tune model hyperparameters, see which performs best\n",
    "2. Perform cross-validation\n",
    "3. Plot ROC curves\n",
    "4. Make a confusion matrix\n",
    "5. Get precision, recall and F1-score metrics\n",
    "6. Find the most important model features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune KNeighborsClassifier (K-Nearest Neighbors or KNN) by hand\n",
    "\n",
    "There's one main hyperparameter we can tune for the K-Nearest Neighbors (KNN) algorithm, and that is number of neighbours. The default is 5 (`n_neigbors=5`).\n",
    "\n",
    "What are neighbours?\n",
    "\n",
    "Imagine all our different samples on one graph like the scatter graph we have above. KNN works by assuming dots which are closer together belong to the same class. If `n_neighbors=5` then it assume a dot with the 5 closest dots around it are in the same class.\n",
    "\n",
    "We've left out some details here like what defines close or how distance is calculated but I encourage you to research them.\n",
    "\n",
    "For now, let's try a few different values of `n_neighbors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of train scores\n",
    "train_scores = []\n",
    "\n",
    "# Create a list of test scores\n",
    "test_scores = []\n",
    "\n",
    "# Create a list of different values for n_neighbors\n",
    "neighbors = range(1, 21) # 1 to 20\n",
    "\n",
    "# Setup algorithm\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Loop through different neighbors values\n",
    "for i in neighbors:\n",
    "    knn.set_params(n_neighbors = i) # set neighbors value\n",
    "    \n",
    "    # Fit the algorithm\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Update the training scores\n",
    "    train_scores.append(knn.score(X_train, y_train))\n",
    "    \n",
    "    # Update the test scores\n",
    "    test_scores.append(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at KNN's train scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are hard to understand, let's plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(neighbors, train_scores, label=\"Train score\")\n",
    "plt.plot(neighbors, test_scores, label=\"Test score\")\n",
    "plt.xticks(np.arange(1, 21, 1))\n",
    "plt.xlabel(\"Number of neighbors\")\n",
    "plt.ylabel(\"Model score\")\n",
    "plt.legend()\n",
    "\n",
    "print(f\"Maximum KNN score on the test data: {max(test_scores)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the graph, `n_neighbors = 11` seems best.\n",
    "\n",
    "Even knowing this, the `KNN`'s model performance didn't get near what `LogisticRegression` or the `RandomForestClassifier` did.\n",
    "\n",
    "Because of this, we'll discard `KNN` and focus on the other two.\n",
    "\n",
    "We've tuned `KNN` by hand but let's see how we can `LogisticsRegression` and `RandomForestClassifier` using [`RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html).\n",
    "\n",
    "Instead of us having to manually try different hyperparameters by hand, `RandomizedSearchCV` tries a number of different combinations, evaluates them and saves the best.\n",
    "\n",
    "### Tuning models with with [`RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n",
    "\n",
    "Reading the Scikit-Learn documentation for [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV), we find there's a number of different hyperparameters we can tune.\n",
    "\n",
    "The same for [`RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n",
    "\n",
    "Let's create a hyperparameter grid (a dictionary of different hyperparameters) for each and then test them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different LogisticRegression hyperparameters\n",
    "log_reg_grid = {\"C\": np.logspace(-4, 4, 20),\n",
    "                \"solver\": [\"liblinear\"]}\n",
    "\n",
    "# Different RandomForestClassifier hyperparameters\n",
    "rf_grid = {\"n_estimators\": np.arange(10, 1000, 50),\n",
    "           \"max_depth\": [None, 3, 5, 10],\n",
    "           \"min_samples_split\": np.arange(2, 20, 2),\n",
    "           \"min_samples_leaf\": np.arange(1, 20, 2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use `RandomizedSearchCV` to try and tune our `LogisticRegression` model.\n",
    "\n",
    "We'll pass it the different hyperparameters from `log_reg_grid` as well as set `n_iter = 20`. This means, `RandomizedSearchCV` will try 20 different combinations of hyperparameters from `log_reg_grid` and save the best ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Setup random hyperparameter search for LogisticRegression\n",
    "rs_log_reg = RandomizedSearchCV(LogisticRegression(),\n",
    "                                param_distributions=log_reg_grid,\n",
    "                                cv=5,\n",
    "                                n_iter=20,\n",
    "                                verbose=True)\n",
    "\n",
    "# Fit random hyperparameter search model\n",
    "rs_log_reg.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_log_reg.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've tuned `LogisticRegression` using `RandomizedSearchCV`, we'll do the same for `RandomForestClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Setup random hyperparameter search for RandomForestClassifier\n",
    "rs_rf = RandomizedSearchCV(RandomForestClassifier(),\n",
    "                           param_distributions=rf_grid,\n",
    "                           cv=5,\n",
    "                           n_iter=20,\n",
    "                           verbose=True)\n",
    "\n",
    "# Fit random hyperparameter search model\n",
    "rs_rf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best parameters\n",
    "rs_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the randomized search random forest model\n",
    "rs_rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! Tuning the hyperparameters for each model saw a slight performance boost in both the `RandomForestClassifier` and `LogisticRegression`.\n",
    "\n",
    "This is akin to tuning the settings on your oven and getting it to cook your favourite dish just right.\n",
    "\n",
    "But since `LogisticRegression` is pulling out in front, we'll try tuning it further with [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n",
    "\n",
    "### Tuning a model with [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "\n",
    "The difference between `RandomizedSearchCV` and `GridSearchCV` is where `RandomizedSearchCV` searches over a grid of hyperparameters performing `n_iter` combinations, `GridSearchCV` will test every single possible combination.\n",
    "\n",
    "In short:\n",
    "* `RandomizedSearchCV` - tries `n_iter` combinations of hyperparameters and saves the best.\n",
    "* `GridSearchCV` - tries every single combination of hyperparameters and saves the best.\n",
    "\n",
    "Let's see it in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different LogisticRegression hyperparameters\n",
    "log_reg_grid = {\"C\": np.logspace(-4, 4, 20),\n",
    "                \"solver\": [\"liblinear\"]}\n",
    "\n",
    "# Setup grid hyperparameter search for LogisticRegression\n",
    "gs_log_reg = GridSearchCV(LogisticRegression(),\n",
    "                          param_grid=log_reg_grid,\n",
    "                          cv=5,\n",
    "                          verbose=True)\n",
    "\n",
    "# Fit grid hyperparameter search model\n",
    "gs_log_reg.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the best parameters\n",
    "gs_log_reg.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "gs_log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we get the same results as before since our grid only has a maximum of 20 different hyperparameter combinations.\n",
    "\n",
    "**Note:** If there are a large amount of hyperparameters combinations in your grid, `GridSearchCV` may take a long time to try them all out. This is why it's a good idea to start with `RandomizedSearchCV`, try a certain amount of combinations and then use `GridSearchCV` to refine them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating a classification model, beyond accuracy\n",
    "\n",
    "Now we've got a tuned model, let's get some of the metrics we discussed before.\n",
    "\n",
    "We want:\n",
    "* ROC curve and AUC score - [`RocCurveDisplay()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.RocCurveDisplay.html) \n",
    "    * **Note:** This was previously `sklearn.metrics.plot_roc_curve()`, as of Scikit-Learn version 1.2+, it is `sklearn.metrics.RocCurveDisplay()`.\n",
    "* Confusion matrix - [`confusion_matrix()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n",
    "* Classification report - [`classification_report()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)\n",
    "* Precision - [`precision_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)\n",
    "* Recall - [`recall_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)\n",
    "* F1-score - [`f1_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)\n",
    "\n",
    "Luckily, Scikit-Learn has these all built-in.\n",
    "\n",
    "To access them, we'll have to use our model to make predictions on the test set. You can make predictions by calling `predict()` on a trained model and passing it the data you'd like to predict on.\n",
    "\n",
    "We'll make predictions on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make preidctions on test data\n",
    "y_preds = gs_log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we've got our prediction values we can find the metrics we want.\n",
    "\n",
    "Let's start with the ROC curve and AUC scores.\n",
    "\n",
    "### ROC Curve and AUC Scores\n",
    "\n",
    "What's a ROC curve?\n",
    "\n",
    "It's a way of understanding how your model is performing by comparing the true positive rate to the false positive rate.\n",
    "\n",
    "In our case...\n",
    "\n",
    "> To get an appropriate example in a real-world problem, consider a diagnostic test that seeks to determine whether a person has a certain disease. A false positive in this case occurs when the person tests positive, but does not actually have the disease. A false negative, on the other hand, occurs when the person tests negative, suggesting they are healthy, when they actually do have the disease.\n",
    "\n",
    "Scikit-Learn implements a function `RocCurveDisplay` (previously called `plot_roc_curve` in Scikit-Learn versions > 1.2) which can help us create a ROC curve as well as calculate the area under the curve (AUC) metric.\n",
    "\n",
    "Reading the documentation on the [`RocCurveDisplay`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.RocCurveDisplay.html) function we can see it has a class method called [`from_estimator(estimator, X, y)`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.RocCurveDisplay.html#sklearn.metrics.RocCurveDisplay.from_estimator) as inputs. \n",
    "\n",
    "Where `estiamator` is a fitted machine learning model and `X` and `y` are the data you'd like to test it on.\n",
    "\n",
    "In our case, we'll use the GridSearchCV version of our `LogisticRegression` estimator, `gs_log_reg` as well as the test data, `X_test` and `y_test`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
